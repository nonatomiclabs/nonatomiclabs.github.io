<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Multichannel Mobile Listening: A State Of The Art — nonatomic.blog</title>
    <meta name="description" content="Title: Multichannel Mobile Listening: A State Of The Art; Date: 2015-02-09; Author: Jean Cruypenynck">
    <meta name="author" content="Jean Cruypenynck">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://www.nonatomiclabs.com/theme/css/ipython.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href="https://www.nonatomiclabs.com/theme/css/local.css" rel="stylesheet">
    <link href="https://www.nonatomiclabs.com/theme/css/pygments.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,300italic,400italic,500,500italic,700,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Ubuntu+Mono' rel='stylesheet' type='text/css'> </head>

<body>
    <div class="container">
        <div class="page-header">
            <a class="fa fa-github fa-2x" style="float: right; margin-top: 10px; margin-right: 2%; font-size: 4vmin; color:black;" href="http://github.com/nonatomiclabs" target="_blank"></a>
            <a class="fa fa-twitter fa-2x" style="float: right; margin-top: 10px; margin-right: 2%; font-size: 4vmin; color: rgb(85, 172, 238);" href="http://twitter.com/nonatomiclabs" target="_blank"></a>
            <h2><a style="float: right; margin-top: 15px; margin-right: 7%;" href="/pages/about.html">About</a></h2>
            <h1><a href="https://www.nonatomiclabs.com/">nonatomic.blog</a>
			<br>            <p class="subtitle">Written by Jean Cruypenynck</p>
        </div>
        <div class="row">
            <div class="col-md-12">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="text-center article-header">
        <h1 itemprop="name headline" class="article-title">Multichannel Mobile Listening: A State Of The Art</h1>
        <time datetime="2015-02-09T00:00:00+01:00" itemprop="datePublished">February 09, 2015</time>
    </div>
    <div>
        Category:
        <span itemprop="articleSection">
			<a href="https://www.nonatomiclabs.com/category/binaural.html" rel="category">Binaural</a>
		</span>
    </div>
     <div itemprop="articleBody" class="article-body"><blockquote>
<p>This article was originally written in May 2014 for the <a href="http://www.iabmfoundation.org/Awards.aspx">IABM Engineering Student Awards</a>.</p>
</blockquote>
<h1>Abstract</h1>
<p>Nowadays, the multiplication of audiovisual contents media such as computers, tablets and smartphones has created a major change in our behavior of content consumption. Broadcasting companies have to adapt their contents to fit our habits.</p>
<p>Binaural and transaural listening are parts of this conversion to mobility.</p>
<h1>Introduction</h1>
<p>Even though binaural recording goes back to 1881 and Clément Adler's <em>Théâtrophone</em>, it is currently expanding. Major broadcasting companies are all thinking about binaural reproduction for a mobile audience.</p>
<p>In fact, we are approaching hyperrealism in increasing the numbers of audio channels and engaging from 2D to 3D in video. These improvements are set up to immerse the audience in the action.</p>
<p>After becoming stereophonic, the sound became multichannel for the first time in 1937 with <em>Fantasia</em> from Walt Disney in quadriphony.  Since this moment, playback has passed by 5.1, 7.1, finally coming to <em>Dolby Atmos</em> and <em>Auro 3D</em>. If 5.1 has now become a standard, there is no doubt about the interest of public in obtaining more and more sound channels. But, having 64 channels as in <em>Dolby Atmos</em> in our room will not became the new standard. Here is the role played by binaural and transaural listening.</p>
<h1>Prerequisites</h1>
<p>This section presents some of the key concepts in mobile multichannel listening.</p>
<h2>Convolution</h2>
<p>As will be explained in next section, convolution is at the heart of both binaural and transaural listening.</p>
<p>Dating from 18th century, convolution is now a widely-used operation in digital signal processing.</p>
<p>If you have never been enlightened as to the joys of convolution, here you can find a short recap of this operation.</p>
<p><strong>Definition:</strong> Let us assume that <span class="math">\(f\)</span> and <span class="math">\(g\)</span> are two functions of a time variable <span class="math">\(t\)</span>. So, the convolution product of <span class="math">\(f\)</span> by <span class="math">\(g\)</span> is defined by:</p>
<div class="math">$$f\left(t\right)\ast g\left(t\right)=\int_{-\infty}^{+\infty}f\left(t\right)g\left(x-t\right)\mathrm{d}t$$</div>
<p>The impulse response of a linear, continuous-time, time-invariant system (afterwards called LTI systems) is defined by the output of the system when it is submitted to a Dirac distribution.</p>
<p>The Dirac distribution is defined as a distribution equal to zero for all <span class="math">\(t\)</span> except zero, equal to <span class="math">\(+\infty\)</span> for <span class="math">\(t=0\)</span> and whose integral is equal to 1.</p>
<p>Knowing the impulse response of a LTI system, we can can convolve it with an input signal to reproduce the behavior of the system. We can also predict the output of this system for a specific input.</p>
<p>An interesting fact in convolution is that the convolution product of two temporal functions is equal to the inverse Fourier transform of the simple product of the two frequency functions or, in other means the Fourier transform of our two temporal functions.</p>
<p>It leads to the following equation:</p>
<div class="math">$$\mathcal F\left[f \left(t \right) * g\left(t \right)\right] = F \left( \nu \right) \times G \left( \nu \right)$$</div>
<p>considering respectively <span class="math">\(\mathcal F\)</span> and <span class="math">\(\mathcal G\)</span> as the Fourier transform of <span class="math">\(f\)</span> and <span class="math">\(g\)</span>.</p>
<p>Without being a main fact of the theorical part, this tip is important in the implementation of convolution, especially in terms of the CPU load.</p>
<h2>The listening process</h2>
<p>The location of sound sources depends on three components:</p>
<ul>
<li>interaural time difference;</li>
<li>interaural level difference;</li>
<li>spectral indices, which are the characteristics of our head (skin, hair, beard…).</li>
</ul>
<p>In fact the acoustic pressure of <em>localized</em> sound source, meaning by this a sound source not placed in the median plan, arrives in a first ear at a certain instant <span class="math">\(t\)</span> with a level <span class="math">\(A\)</span> and in the second ear at the instant <span class="math">\(t+\delta t\)</span> at an amplitude of <span class="math">\(A−\delta A\)</span>.</p>
<p><img alt="Interaural Time Difference" src="https://www.nonatomiclabs.com/blog/2015/02/09/itd.png" /></p>
<p>According to the previous figure, the interaural time difference is equal to <span class="math">\(r \left( \sin\theta +\theta \right)\)</span>. The interaural level difference would be equal to the <span class="math">\(\delta A\)</span> mentionned before.</p>
<h1>About binaural</h1>
<p>Binaural sound has the ambition to reproduce a three-dimensionnal sound field with two in-ear audio channels only. This technology, although seeming innovative is not new and is based on simple physical principles.</p>
<p><strong>Acquisition of the BRIR:</strong> The principle is to record the result of a known test signal reproduced by a spatialized sound source in each ear. Little electret condenser microphones placed in ears are often used to achieve these measurements. After being recorded, the signal is deconvoluted in an impulse response.</p>
<p>The group of impulse responses obtained by repeating the operation for each channel of a multichannel system is known as <em>Brain Related Impulse Response</em> (BRIR). From BRIR, we can easily extract the <em>Head Related Transfer Function</em> (HRTF), in fact in going in frequency domain by Fourier transform.</p>
<p><strong>Use of the BRIR:</strong> When the BRIR is obtained, each channel of a signal is convoluted with its corresponding left and right impulse responses, as shown on the figure below. The output signal is therefore a stereophonic signal reproducing a multichannel system.</p>
<p><img alt="Multiple source binaural spatializer" src="https://www.nonatomiclabs.com/blog/2015/02/09/binaural-convolution.png" /></p>
<p><strong>Main issue:</strong> The main issue in binaural listening is the unique nature of each BRIR. In fact, the BRIR depends on characteristics of our head. Sound sources localization can be very altered in a binaural signal made with a generic BRIR.</p>
<p><strong>Test signals:</strong> Test signals used to measure BRIR are essentially the same as in reverb impulse response measurements. Nowadays, it appears that the exponential sine sweep signal still seems the best in terms of fidelity and signal-to-noise ratio. This signal is defined by:</p>
<div class="math">$$x \left( t \right) = \sin\left[\frac{\omega_1 T}{\ln \left( \frac{\omega_1}{\omega_2}\right)}\left(\mathrm e ^{\frac{t}{T} \ln \left( \frac{\omega_2}{\omega_1}\right)}-1\right)\right]$$</div>
<h1>About transaural</h1>
<p>The founding principles of transaural are the same as binaural. The difference between both is in the playback means. In the case of transaural, we use external speakers while in binaural we use headphones.</p>
<p>In concrete terms, transaural listening reproduces multichannel listening in a specific room with two speakers, using the transfer function of the room.</p>
<p>This is achieved with cross-talk cancelling. Traditionnally, when listening with speakers, the left signal goes to the left ear and a little bit to the right ear and conversely. In transaural, we filter left and right signals to properly separate the signals. After obtaining the cross-talk cancellation function, it only remains to put binaural signals in input and the signal at the ears will be spatialized.</p>
<p><img alt="General transaural filter" src="https://www.nonatomiclabs.com/blog/2015/02/09/transaural-filter.png" /></p>
<p>In transaural, the main problem is the unique nature of the transfer functions of the rooms.</p>
<h1>Possible Future Evolutions</h1>
<p>Without mating wild guesses, we already can think about believable audio innovations to revolutionise the current situation.</p>
<p><strong>Recognition enhancements:</strong> Firstly, recognition of spectral indices and rooms. It would put aside the uniqueness of:</p>
<ul>
<li>each BRIR;</li>
<li>rooms in which transaural content would be listened to.</li>
</ul>
<p>For the BRIR, two ways seem to emerge:</p>
<p><em>Using different morphotypes to categorize most of the population.<br />
About ten morphotypes appear to be enough; the user would be asked to describe their face and the software would build a specific IR adapted to them.
* Using pictures to create a 3D map of the subject's face and design their BRIR. </em>Thirdlove* company has already launched a mobile application for women to find their real bra size with only two pictures. With maybe, let's say, five pictures of our face, it would be possible to get an accurate 3D map.</p>
<p><em>Google</em> is currently developping a technology called <em>Tango</em>: it makes it possible to create a 3D map of our environment from our smartphone or tablet. Combined to the accelerometer and gyroscope which have already been present for a couple of years, we could get a tablet which adapts the sound output according to the position in space to create a transaural result. Of course, there would be other problems in this case like lack of acoustical power and informations about room materials.</p>
<p><strong>Access enhancements:</strong> We could think about a cloud platform like <em>Dropbox</em> containing our personal BRIR. When taking the tablet of a friend, we would just have to log into our account and access our BRIR to listen to multichannel content on their device.</p>
<p>In terms of production and broadcast, it would be possible to send the user an MPEG Surround stream which would be decoded into binaural content if headphones are plugged in, into transaural content if using the speakers of a mobile device, or into classical multichannel content matching the actual configuration of the user, <em>i.e.</em> 7.1 to 5.1.</p>
<p><strong>Algorithm enhancements:</strong> Finally, I wrote before that the impulse response of a LTI system makes it possible to oversee the output of this system knowing the input. But our listening process can not really be assimilated to a linear system. So we can also imagine an enhancement of the binaural/transaural transcoding in this way: non-linear convolution and Volterra series could be the basis of this tweak. These operations will not be seen in this paper but I invite the reader to find out more about them.</p>
<h1>Conclusion</h1>
<p>Technology allows us to achieve more and more complex processes and we have to take advantage of that. Now that our computers are able to process real-time convolutions, there just remains the issue of personalized BRIR and rooms.</p>
<p>For the first, it would be possible to create enough categories to fit the vast majority of users or to determine each BRIR via 3D modelisation based on pictures of our face.</p>
<p>For the second, we have to use waves to recreate the architecture of the room and use motion sensors and near field information to know the exact place of the user.</p>
<p>Although both binaural and transaural listening seem far from us, I think the two methods are a future logical implementation. Binaural personalisation does not seem too hard to create whereas the transaural modelisation of a room would take more years to develop.</p>
<p><a href="https://www.nonatomiclabs.com/download/Multichannel Mobile Listening - A State of the Art.pdf">Download this article.</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <hr>
    <h2>Comments</h2> <a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="nonatomiclabs">Tweet</a>
<script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
 <div id="disqus_thread"></div>
<script type="text/javascript">
var disqus_shortname = 'nonatomic.blog';
var disqus_title = 'Multichannel Mobile Listening: A State Of The Art';

(function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = "https://' + disqus_shortname + '.disqus.com/embed.js";
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
            </div>
        </div>         <!-- <hr> -->
    </div>
    <!-- /container -->
    <div class="container">
        <div class="row">
            <div class="col-md-12 text-center center-block aw-bottom">
                <p>&copy; Jean Cruypenynck 2015&mdash;2020</p>
            </div>
        </div>
    </div>
    <!-- JavaScript -->
    <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script type="text/javascript">
    jQuery(document).ready(function($) {
        $("div.collapseheader").click(function() {
            $header = $(this).children("span").first();
            $codearea = $(this).children(".input_area");
            $codearea.slideToggle(500, function() {
                $header.text(function() {
                    return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
                });
            });
        });
    });
    </script>
<script>
(function(i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

ga('create', 'UA-64506480-3', 'auto');
ga('send', 'pageview');
</script>
 </body>

</html>